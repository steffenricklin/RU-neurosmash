{"cells":[{"cell_type":"markdown","source":"# Convolutional Auto-Encoder","metadata":{"deepnote_cell_type":"markdown","cell_id":"00000-89420bb6-4667-4bfe-8da6-28fbf547e761"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00001-27162a39-c57b-43ca-8b46-2727acc9eb7b","output_cleared":false,"source_hash":"a78aefca","execution_millis":0,"execution_start":1605602641219},"source":"# Imports\nfrom convvae import ConvVae\nimport numpy as np\nimport PIL\nfrom sklearn.model_selection import train_test_split","execution_count":8,"outputs":[]},{"cell_type":"code","source":"import Neurosmash\n\n# These are the default environment arguments. They must be the same as the values that are set in the environment GUI.\nip         = \"127.0.0.1\" # Ip address that the TCP/IP interface listens to (127.0.0.1 by default)\nport       = 13000       # Port number that the TCP/IP interface listens to (13000 by default)\n\n# This is the size of the texture that the environment is rendered.\n# This is set to 784 by default, which will result in a crisp image but slow speed.\n# You can change the size to a value that works well for your environment but should not go too low.\nsize       = 64# 96, 192\n\n# This is the simulation speed of the environment. This is set to 1 by default.\n# Setting it to n will make the simulation n times faster.\n# In other words, less (if n < 1) or more (if n > 1) simulation time will pass per step.\n# You might want to increase this value to around 10 if you cannot train your models fast enough\n# so that they can sample more states in a shorter number of steps at the expense of precision.\ntimescale  = 5\n\n# This is an example agent.\n# It has a step function, which gets reward/state as arguments and returns an action.\n# Right now, it always outputs a random action (3) regardless of reward/state.\n# The real agent should output one of the following three actions:\n# none (0), left (1) and right (2)\nagent = Neurosmash.Agent() \n\n# This is the main environment.\n# It has a reset function, which is used to reset the environment before episodes.\n# It also has a step function, which is used to which steps one time point\n# It gets an action (as defined above) as input and outputs the following:\n# end (true if the episode has ended, false otherwise)\n# reward (10 if won, 0 otherwise)\n# state (flattened size x size x 3 vector of pixel values)\n# The state can be converted into an image as follows:\n# image = np.array(state, \"uint8\").reshape(size, size, 3)\n# You can also use to Neurosmash.Environment.state2image(state) function which returns\n# the state as a PIL image\nenvironment = Neurosmash.Environment(ip, port, size, timescale) \n\n","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00002-a7f4647d-f5ac-4848-aefa-56e6dd563f8c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The following steps through an entire episode from start to finish with random actions (by default)\n\nend, reward, state = environment.reset()\nnr_images = 1000\nshape = np.array(state).reshape((size,size,3)).shape\nimages = np.zeros((nr_images, shape[0], shape[1], shape[2]))\n\ni = 0\nwhile i < nr_images:\n    end, reward, state = environment.reset()\n    while (end == 0):\n        action = agent.step(end, reward, state)\n        end, reward, state = environment.step(action)\n        images[i] = np.array(state).reshape(shape)\n        i += 1\n        if i >= nr_images:\n            end = 1\n\n    # Let's run it a few more steps so that the things have time to settle down\n    for _ in range(100):\n        if i >= nr_images:\n            continue\n        action = agent.step(end, reward, state)\n        end, reward, state = environment.step(action)\n        images[i] = np.array(state).reshape(shape)\n        i += 1\n\nenvironment.state2image(state)","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-f715da44-dc77-493c-a2e8-16ee7023683a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_iter = mx.io.NDArrayIter(data={'data': train_data}, batch_size = batch_size)\ntest_iter = mx.io.NDArrayIter(data={'data': test_data}, batch_size = batch_size)\n\nvae = ConvVae()\nvae.initialize()\nvae.hybridize()\ntrainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': .001})","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-882c3f59-c6b5-4e03-98be-eceef2911084"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_vae(n_epochs):\n    train_loss = []\n    valid_loss = []\n\n    for epoch in n_epochs:\n        ","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-8f475e7b-0ea8-4cd4-863d-5d66f0c87543"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"deepnote_notebook_id":"d560aee3-cb13-400a-9cbc-b5a7d67e48fb","deepnote_execution_queue":[]}}